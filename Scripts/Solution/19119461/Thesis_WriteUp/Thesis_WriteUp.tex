\documentclass[11pt,preprint, authoryear]{elsarticle}

\usepackage{lmodern}
%%%% My spacing
\usepackage{setspace}
\setstretch{1.2}
\DeclareMathSizes{12}{14}{10}{10}

% Wrap around which gives all figures included the [H] command, or places it "here". This can be tedious to code in Rmarkdown.
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}

\let\origtable\table
\let\endorigtable\endtable
\renewenvironment{table}[1][2] {
    \expandafter\origtable\expandafter[H]
} {
    \endorigtable
}


\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi

\usepackage{amssymb, amsmath, amsthm, amsfonts}

\def\bibsection{\section*{References}} %%% Make "References" appear before bibliography


\usepackage[round]{natbib}

\usepackage{longtable}
\usepackage[margin=2.3cm,bottom=2cm,top=2.5cm, includefoot]{geometry}
\usepackage{fancyhdr}
\usepackage[bottom, hang, flushmargin]{footmisc}
\usepackage{graphicx}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1.3ex plus 0.5ex minus 0.3ex}
\usepackage{textcomp}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.3pt}

\usepackage{array}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}

%%%%  Remove the "preprint submitted to" part. Don't worry about this either, it just looks better without it:
\makeatletter
\def\ps@pprintTitle{%
  \let\@oddhead\@empty
  \let\@evenhead\@empty
  \let\@oddfoot\@empty
  \let\@evenfoot\@oddfoot
}
\makeatother

 \def\tightlist{} % This allows for subbullets!

\usepackage{hyperref}
\hypersetup{breaklinks=true,
            bookmarks=true,
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=blue,
            pdfborder={0 0 0}}


% The following packages allow huxtable to work:
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{caption}


\newenvironment{columns}[1][]{}{}

\newenvironment{column}[1]{\begin{minipage}{#1}\ignorespaces}{%
\end{minipage}
\ifhmode\unskip\fi
\aftergroup\useignorespacesandallpars}

\def\useignorespacesandallpars#1\ignorespaces\fi{%
#1\fi\ignorespacesandallpars}

\makeatletter
\def\ignorespacesandallpars{%
  \@ifnextchar\par
    {\expandafter\ignorespacesandallpars\@gobble}%
    {}%
}
\makeatother

\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{CSLReferences}%
  {\setlength{\parindent}{0pt}%
  \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces}%
  {\par}


\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

%%% Include extra packages specified by user
\usepackage{array}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage[normalem]{ulem}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{hyperref}

%%% Hard setting column skips for reports - this ensures greater consistency and control over the length settings in the document.
%% page layout
%% paragraphs
\setlength{\baselineskip}{12pt plus 0pt minus 0pt}
\setlength{\parskip}{12pt plus 0pt minus 0pt}
\setlength{\parindent}{0pt plus 0pt minus 0pt}
%% floats
\setlength{\floatsep}{12pt plus 0 pt minus 0pt}
\setlength{\textfloatsep}{20pt plus 0pt minus 0pt}
\setlength{\intextsep}{14pt plus 0pt minus 0pt}
\setlength{\dbltextfloatsep}{20pt plus 0pt minus 0pt}
\setlength{\dblfloatsep}{14pt plus 0pt minus 0pt}
%% maths
\setlength{\abovedisplayskip}{12pt plus 0pt minus 0pt}
\setlength{\belowdisplayskip}{12pt plus 0pt minus 0pt}
%% lists
\setlength{\topsep}{10pt plus 0pt minus 0pt}
\setlength{\partopsep}{3pt plus 0pt minus 0pt}
\setlength{\itemsep}{5pt plus 0pt minus 0pt}
\setlength{\labelsep}{8mm plus 0mm minus 0mm}
\setlength{\parsep}{\the\parskip}
\setlength{\listparindent}{\the\parindent}
%% verbatim
\setlength{\fboxsep}{5pt plus 0pt minus 0pt}



\begin{document}



%titlepage
\thispagestyle{empty}
\begin{center}
\begin{minipage}{0.75\linewidth}
    \centering
%Entry1
    {\uppercase{\huge Correlating Factors of U.S. Presidential Speeches
with Stock Market Movements: a Machine Learning Approach.\par}}
    \vspace{2cm}
%Author's name
    {\LARGE \textbf{Pablo Rees}\footnote{\textbf{Acknowledgements:}
  \newline \emph{I would like to thank the Stellenbosch University PSP
  foundation for sponsorship of my Masters Degree and this project by
  extension. I would also like to thank Dr Dawie van Lill for the
  provision his wealth of knowledge and resources in Machine and Deep
  Learning subject matter without which this project would have been
  impossible and finally the Stellenbosch Economics department as a
  whole for it's liberal approach to engagement with students on
  academic and administrative issues with special mention of Dr Gideon
  du Randt, Nico Katzke and Carina Smit for support along the way.}}\par}
    \vspace{1cm}
%University logo
%Supervisor's Details
\begin{center}
    {\Large Thesis presented in fulfilment of the requirements for the
degree of Master of Economics in the Faculty of Economics at
Stellenbosch University.\par}
    \vspace{1cm}
%Degree
    {\large Supervisor: \vfill Dr Dawie van Lill\par}
    \vspace{1cm}
%Institution
    {\large December 2022\par}
    \vspace{1cm}
%Date
    {\large }
%More
    {\normalsize }
%More
    {\normalsize }
\end{center}
\end{minipage}
\end{center}
\clearpage


\begin{frontmatter}  %

\title{}

% Set to FALSE if wanting to remove title (for submission)

\begin{abstract}
\small{
Less than 500 words

Purpose and motivation:

Problem statement: The purpose of this study is to\ldots{}

Methods: Time series, NLP, comparison of ML prediction and regression
between datasets that contain NLP variables and those that don't

Results:

Conclusion: What do the results mean in the context of the problem? What
unanswered questions remain? What other studies might yield solutions to
these problems?

Abstract to be written here. The abstract should not be too long and
should provide the reader with a good understanding what you are writing
about. Academic papers are not like novels where you keep the reader in
suspense. To be effective in getting others to read your paper, be as
open and concise about your findings here as possible. Ideally, upon
reading your abstract, the reader should feel he / she must read your
paper in entirety.
}
\end{abstract}

\vspace{1cm}


\begin{keyword}
\footnotesize{
S\&P 500 \sep Machine Learning \sep Natural Language Processing
\sep Presidential Speeches \sep Time Series Econometrics \\
\vspace{0.3cm}
}
\footnotesize{
\textit{JEL classification} G17
}
\end{keyword}



\vspace{0.5cm}

\end{frontmatter}


\renewcommand{\contentsname}{Table of Contents}
{\tableofcontents}

%________________________
% Header and Footers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\chead{}
\rhead{}
\lfoot{}
\rfoot{\footnotesize Page \thepage}
\lhead{}
%\rfoot{\footnotesize Page \thepage } % "e.g. Page 2"
\cfoot{}

%\setlength\headheight{30pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%________________________

\headsep 35pt % So that header does not go over title




\newpage

\hypertarget{introduction}{%
\section{\texorpdfstring{Introduction
\label{Introduction}}{Introduction }}\label{introduction}}

\begin{quote}
I suggest renaming the top line after @article, as done in the template
ref.bib file, to something more intuitive for you to remember. Do not
change the rest of the code. Also, be mindful of the fact that bib
references from google scholar may at times be incorrect. Reference
Latex forums for correct bibtex notation.
\end{quote}

\hypertarget{data}{%
\section*{Data}\label{data}}
\addcontentsline{toc}{section}{Data}

Notice how I used the curly brackets and dash to remove the numbering of
the data section.

\hypertarget{literature-review}{%
\section{\texorpdfstring{Literature review
\label{Literature review}}{Literature review }}\label{literature-review}}

The aim of this project is to determine whether U.S Presidential
speeches have predictive power over stock market movements. A positive
result would be powerful and could add to the ability of traders to
accurately predict stock market movements. Broadly, machine learning has
been selected as the method of modelling and the S\&P 500 index has been
chosen as representative of the `stock market'.

The undertaking of this project assumes 2 important conjectures. First,
that there is a correlation between the linguistic factors\footnote{`Linguistic
  factors' in this sense is intended to mean any and all patterns that
  can be detected in spoken language including verbiage, lexicon, tone,
  register, sentence length, word combination etc.} of speech employed
by U.S. presidents in their speeches and U.S. stock market movements;
and second, that the prominence of speech factors can be quantified by
using machine learning algorithms. The following literature review
defends the two conjectures, confirms the novelty of the project, and
surveys methods of data cleaning and analysis to discover which machine
learning methods are most appropriate for this project.

\hypertarget{conjecture-defence}{%
\subsection{\texorpdfstring{Conjecture defence
\label{Conjecture defence}}{Conjecture defence }}\label{conjecture-defence}}

The following section references the findings of four peer-reviewed
articles in defence of the conjectures made in section 1. More evidence
exists but is not necessary to sufficiently defend the conjectures made
here.

\hypertarget{fomc-speeches-and-u.s.-financial-market-reactions}{%
\subsubsection{\texorpdfstring{FOMC speeches and U.S. financial market
reactions
\label{FOMC speeches}}{FOMC speeches and U.S. financial market reactions }}\label{fomc-speeches-and-u.s.-financial-market-reactions}}

\protect\hyperlink{ref-hayo2008communicating}{Hayo, Kutan \& Neuenkirch}
(\protect\hyperlink{ref-hayo2008communicating}{2008: 27}) performed a
generalized auto-regressive conditional heteroscedastic (GARCH) analysis
of the relationship between Federal Open Market Committee (FOMC)
speeches and the U.S. financial market. The analysis was quantitative on
the market side and qualitative on the speech side. They found that FOMC
speeches influence trader behaviour, but that this effect is both
asymmetrical (negative impacts were larger than positive impacts) and
non-uniform across trader type (bond markets were affected far more than
financial and forex markets). Further, more formal modes of
communication have a larger impact on both returns and conditional
variance and more prominent speakers have a greater impact on bond
markets. Finally, they found that volatility in 3- and 6-month T-bills
was reduced on the day of a speech.

It was commented that heteroskedasticity left something to be desired
when assessing the effects of monetary shocks. Further, it was found
that speeches alone were not sufficient to create significant effects
for financial markets. It is important that news agencies propagate the
news for market repercussions to occur. In a brief, informal interview
with a few bond traders it was discovered that they tended to ``read
monetary policy statements and listen to speeches by Greenspan
(Bernanke) themselves. Other types of communications are rather
neglected and the traders tend to rely on newswire information''
(\protect\hyperlink{ref-hayo2008communicating}{Hayo \emph{et al.},
2008}). Further, it was noted that news articles fail to take a neutral
stance on the contents of speeches implying that the market effect may
be distorted by the sentiment of news agencies.

This article shows that the sentiments of communications influence the
effect of those communications on markets. Thus, analysing sentiment is
an important factor in an accurate appraisal of the relationship between
speeches and markets. The finding that speeches need to be propagated by
news agencies in order to have significant impacts on financial markets
is counter to the hypothesis of this project; however the FOMC is less
publicly scrutinized than a U.S. president which may render this finding
only mildly significant to this project.

\hypertarget{political-speeches-and-stock-market-outcomes}{%
\subsubsection{\texorpdfstring{Political speeches and stock market
outcomes
\label{Political speeches}}{Political speeches and stock market outcomes }}\label{political-speeches-and-stock-market-outcomes}}

\protect\hyperlink{ref-maligkris2017political}{Maligkris}
(\protect\hyperlink{ref-maligkris2017political}{2017}) demonstrates that
the speeches given by U.S. presidential candidates directly influence
the stock market, particularly during the early months of their
campaigns. These speeches often contain information about potential
presidents' positions on policy changes and public issues. Thus, they
can affect investor sentiment and, in turn, the stock market. The
employed methodology was to analyse transcripts of presidential
candidates' speeches from the American Presidency Project archives and
the U.S. Government Publishing Project from the 2004-2016 period
according to the index developed in
\protect\hyperlink{ref-baker2016measuring}{Baker, Bloom \& Davis}
(\protect\hyperlink{ref-baker2016measuring}{2016}) (explained in section
\ref{policy uncertainty}). He shows, using regression analysis that
there is an average increase in excess market returns of 26 basis points
following candidate speeches, however the direction and magnitude of
this effect varies between candidates. He goes on to examine whether the
difference in effect is due to heterogenous speech content. Finally, it
was demonstrated that speeches laden with economic information tend to
boost stock returns while also reducing volatility. Speeches with a
negative tone have the opposite effect and the long run effect of
speeches is dependent on market conditions.

This paper demonstrates that there is a correlation between presidential
candidates' speeches and stock movements. It is then reasonable to
believe that there is also a correlation between presidents' speeches
and stock market movements. It also highlights that tone affects the
relationship and thus that the sentiment of a speech is important.
Further, the the paper provides a perfect resource for gathering U.S
presidential speech data - the American Presidency Project.

\hypertarget{measuring-economic-policy-uncertainty}{%
\subsubsection{\texorpdfstring{Measuring economic policy uncertainty
\label{policy uncertainty}}{Measuring economic policy uncertainty }}\label{measuring-economic-policy-uncertainty}}

\protect\hyperlink{ref-baker2016measuring}{Baker \emph{et al.}}
(\protect\hyperlink{ref-baker2016measuring}{2016}) develop an Economic
Policy Uncertainty Index based on the frequency of articles containing a
trio of terms in 10 leading U.S. newspapers. These terms were: `
``economic'' or ``economy'' ; ``uncertain'' or ``uncertainty''; and one
or more of ``congress'', ``deficit'', ``Federal Reserve'',
`legislation'', ``regulation'', or ``White House''\,'
(\protect\hyperlink{ref-baker2016measuring}{Baker \emph{et al.}, 2016:
1594}). The terms were selected over a period of 24 months during which
more than 15 000 news articles were read by humans in an auditing
process. The index proved to be quite accurate, spiking near the
expected events, including wars, tight presidential elections, fiscal
policy battles and terrorist activity.

They went on to show that their index had a strong relationship with
other economic uncertainty measures and policy uncertainty measures.
Further, congruence in uncertainty prediction was found between left-
and right- leaning newspapers.

This article shows that language processing can be used to predict
economic events, particularly economic uncertainty and economic policy
uncertainty. However, the type of language processing proposed for this
project differs significantly. While
\protect\hyperlink{ref-baker2016measuring}{Baker \emph{et al.}}
(\protect\hyperlink{ref-baker2016measuring}{2016}) used man hours; this
project uses machine learning.

\hypertarget{hope-change-and-financial-markets-can-obamas-words-drive-the-market}{%
\subsubsection{\texorpdfstring{Hope, change and financial markets: can
Obama's words drive the market?
\label{Obama}}{Hope, change and financial markets: can Obama's words drive the market? }}\label{hope-change-and-financial-markets-can-obamas-words-drive-the-market}}

\protect\hyperlink{ref-sazedj2011hope}{Sazedj \& Tavares}
(\protect\hyperlink{ref-sazedj2011hope}{2011}) asked whether the
speeches made by former U.S. President Barack Obama affected stock
market prices. By regressing the event of a speech during Obama's first
11 months in office on the daily excess returns of the Dow Jones, the
S\&P 500 and the NASDAQ they found that the event of a speech had a
generally insignificant effect on daily excesses. However, by regressing
key terms contained in 43 speeches given during the first 11 months of
Obama's presidency it was found that the content of speeches can
significantly affect daily excess returns nearly uniformly across all
three indices. Notably, the NASDAQ's correlation to the content of
speeches was weaker -- indicating that technology markets may be less
susceptible to presidential rhetoric.

This paper highlights the relationship between the content of a
presidential speech and stock market returns. However, this study was
correlational rather than predictive in nature and therefore does not
offer a conclusion on the hypothesis of this project

\hypertarget{conjecture-defence-summary}{%
\subsubsection{\texorpdfstring{Conjecture defence summary
\label{defence summary}}{Conjecture defence summary }}\label{conjecture-defence-summary}}

As seen in section \ref{Political speeches} and section \ref{Obama} it
is true that there is a correlation between the linguistic factors of
speech employed by U.S. presidents in their speeches and U.S. stock
market movements. Further, section \ref{FOMC speeches} and section
\ref{policy uncertainty} show that sentiment and other linguistic
factors of speech can be quantified using machine learning methods.
Thus, both conjectures hold and further investigation is warranted. This
is not the total of all evidence supporting these conjectures but is
sufficient. Other articles reviewed here can be seen for further
evidence.

\hypertarget{novelty}{%
\subsection{\texorpdfstring{Novelty
\label{novelty}}{Novelty }}\label{novelty}}

Searching `sentiment analysis stock market' on Google Scholar yields
mostly articles linking Twitter data and the stock market. Searching
`presidential speeches affect stock market' on Google Scholar yields
articles on the relationship between presidential speeches and the stock
market but none using Machine Learning techniques.
\protect\hyperlink{ref-khedr2017predicting}{Khedr, Yaseen \& others}
(\protect\hyperlink{ref-khedr2017predicting}{2017}) describe related
work as including relating news or Twitter data to stock market behavior
and prices and relating financial news to stock prices, but do not
mention presidential speeches. Searching `machine learning S\&P 500' on
Google Scholar yields an array of articles using machine learning as a
technical analysis tool but most of these use other financial indicators
and are not NLP based -- bar one article analysing the effects of former
U.S. president, Donald Trump's tweets on the S\&P 500 and the DJIA.
Searching `political speech machine learning' on Google Scholar yields
articles that focus on political speeches but have no link to stock
markets. Other searches yielded similar results, thus as far as can be
told -- this research is novel in nature.

\hypertarget{data-cleaning-methods-for-nlp}{%
\subsection{\texorpdfstring{Data cleaning methods for NLP
\label{data cleaning}}{Data cleaning methods for NLP }}\label{data-cleaning-methods-for-nlp}}

\protect\hyperlink{ref-katre2019nlp}{Katre}
(\protect\hyperlink{ref-katre2019nlp}{2019}), in his analysis of Indian
political speeches, uses the Natural Language Toolkit (NLTK) package and
string methods to remove punctuation, HTML tags and English
stopwords\footnote{`Stopwords' are words that commonly occur across all
  speech and therefore only create noise in the data. Some examples are
  `the', `it', `they' and `and'.}, as well as converting speeches to
lowercase and tokenizing\footnote{`Tokenizing' refers to the splitting
  of words into tokens that have linguistic importance, for example the
  words `terrorism', `terrorist' and `terror' may all be tokenized to
  `terror'. Thus, the core concept of the word is captured while also
  simplifying the dataset.} them.
\protect\hyperlink{ref-zubair2015extracting}{Zubair \& Cios}
(\protect\hyperlink{ref-zubair2015extracting}{2015}), before correlating
the sentiment in Reuters articles with S\&P 500 movements, clean their
text data by tokenizing it with NLTK.
\protect\hyperlink{ref-kinyua2021analysis}{Kinyua, Mutigwe, Cushing \&
Poggi} (\protect\hyperlink{ref-kinyua2021analysis}{2021}) clean their
Twitter data (tweets from then U.S. president, Donald Trump) by deleting
all tweets on days when the stock trading was closed, deleting all
tweets that only contained standard stopwords and deleting all tweets
that only contained URLs.
\protect\hyperlink{ref-khedr2017predicting}{Khedr \emph{et al.}}
(\protect\hyperlink{ref-khedr2017predicting}{2017}) tokenize,
standardize by converting to lowercase, remove stopwords from and
stem\footnote{`Stemming' refers to the removal of suffixes to reduce the
  complexity of a dataset.} their textual data before processing the
abbreviations (replacing abbreviations with the full phrase) and
filtering out words that consist of two or less characters.

\hypertarget{machine-learning-methods}{%
\subsection{\texorpdfstring{Machine learning methods
\label{ML}}{Machine learning methods }}\label{machine-learning-methods}}

The following section looks first at the literature informing the
sentiment analysis space and then at the literature around stock market
prediction in order to determine methods that suit the intersection
between the two.

\hypertarget{sentiment-analysis-methods}{%
\subsubsection{\texorpdfstring{Sentiment analysis methods
\label{sentAnal}}{Sentiment analysis methods }}\label{sentiment-analysis-methods}}

\protect\hyperlink{ref-ren2018forecasting}{Ren, Wu \& Liu}
(\protect\hyperlink{ref-ren2018forecasting}{2018}) analyse news articles
at the sentence level by assigning a sentiment polarity (using software
designed for the Chinese language) to each word followed by a sentiment
score for each sentence in a document. Each document is then categorized
and a sentiment score between -1 and 1 for all news for each day is
generated. \protect\hyperlink{ref-zubair2015extracting}{Zubair \& Cios}
(\protect\hyperlink{ref-zubair2015extracting}{2015}) use the positive
and negative valence categories from the Harvard General Enquirer (HGI)
to assign each word in a Reuters news article a positive or negative
label. They then sum the positives and negatives into a tuple and divide
that tuple by the number of words in an article in order to create a
vector that represents each news article. The vectors are organized into
time series, normalized by dividing all vectors by the first vector,
parsed through a Kalman filter and then correlated to S\&P 500 returns
using Pearson correlation (for both the positive and negative scalar in
the vector). \protect\hyperlink{ref-kinyua2021analysis}{Kinyua \emph{et
al.}} (\protect\hyperlink{ref-kinyua2021analysis}{2021}) use the Valence
Aware Dictionary for Sentiment Reasoning (VADER) to create a sentiment
feature for former U.S. president Donald Trump's tweets which was then
used as a regression feature in linear, decision tree and random forest
regressions. \protect\hyperlink{ref-khedr2017predicting}{Khedr \emph{et
al.}} (\protect\hyperlink{ref-khedr2017predicting}{2017}) use N-gram
(n=2) to extract key phrases from their corpus of news text data, then
term-frequency inverse-document-frequency (TF-IDF) is used to determine
the importance of those phrases within the corpus, and finally use a
naïve-Bayes classifier to assign positive and negative labels to each
news document. \protect\hyperlink{ref-purevdagva2020machine}{Purevdagva,
Zhao, Huang \& Mahoney}
(\protect\hyperlink{ref-purevdagva2020machine}{2020}) use a variety of
features present in both data and metadata to predict fake political
speech. Two features relevant to this project were `speaker job' and
`context' (press, direct or social) which were labelled using universal
serial encoders. For the actual sentiment analysis they used the
linguistic inquiry and word count (LIWC) tool to categorize and count
words into emotional, cognitive and structural text components. Various
further attempts to extract sentiment from the text did not yield
increased prediction accuracies. They go on to use an extra tree
classifier for feature selection and then support vector machine (SVM),
multilayer perceptron, convolutional neural network (CNN), decision
trees, fasttext and bidirectional encoder representations from
transformers (BERT) for prediction with the highest accuracy resulting
from the SVM. \protect\hyperlink{ref-dilaisentiment}{Dilai, Onukevych \&
Dilai} (\protect\hyperlink{ref-dilaisentiment}{2021}) use SentiStrength
-- an automatic sentiment analysis tool - to compare the sentiment in
speeches between former U.S. president Donald Trump and former Ukrainian
president Petro Poroshenko.

\hypertarget{stock-market-prediction-methods}{%
\subsubsection{\texorpdfstring{Stock market prediction methods
\label{stockPred}}{Stock market prediction methods }}\label{stock-market-prediction-methods}}

\protect\hyperlink{ref-ren2018forecasting}{Ren \emph{et al.}}
(\protect\hyperlink{ref-ren2018forecasting}{2018}) use an SVM and
five-fold cross validation approach to achieve a prediction accuracy of
98\% when predicting fake news in political speech. They combined
sentiment data and market indicators as their input data.
\protect\hyperlink{ref-kinyua2021analysis}{Kinyua \emph{et al.}}
(\protect\hyperlink{ref-kinyua2021analysis}{2021}) use linear, decision
tree and random forest regressions to predict S\&P 500 and DJIA
directional changes. Random forest regression performed best for both
datasets. \protect\hyperlink{ref-khedr2017predicting}{Khedr \emph{et
al.}} (\protect\hyperlink{ref-khedr2017predicting}{2017}) use open,
high, low and close (OHLC) prices and the first lag of directional
change as features for prediction of future market trends.
\protect\hyperlink{ref-jiao2017predicting}{Jiao \& Jakubowicz}
(\protect\hyperlink{ref-jiao2017predicting}{2017}) extracted lag and
window features from the S\&P 500 and the global 8 index before running
time series random forest, neural network and gradient boosted trees to
predict movements of individual stocks in the S\&P 500.
\protect\hyperlink{ref-liu2016forecasting}{Liu, Wang, Xiao \& Liang}
(\protect\hyperlink{ref-liu2016forecasting}{2016}) used forward search
feature selection to select features for SVM, naïve-Bayes, Gaussian
discriminant analysis and logistic regression from a set of economic
features including the crude oil daily return, currency exchange rates
and major stock indices daily returns in order to forecast the S\&P 500
movement.

\hypertarget{summary-of-literature-review}{%
\subsection{\texorpdfstring{Summary of literature review
\label{litSumm}}{Summary of literature review }}\label{summary-of-literature-review}}

Table 2.1 represents the literature review in a condensed format which
allows for easy comparison of the data and methods used and resulting
accuracies. Table 2.2 represents the metadata, linked through `Paper
number' for Table 2.1.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.07}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.26}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.31}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.17}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.13}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.07}}@{}}
\caption{Condensed literature review}\tabularnewline
\toprule
\begin{minipage}[b]{\linewidth}\centering
Paper number
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
ML Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Cleaning method
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Data type
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Index predicted
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Max accuracy
\end{minipage} \\
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{\linewidth}\centering
Paper number
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
ML Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Cleaning method
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Data type
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Index predicted
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Max accuracy
\end{minipage} \\
\midrule
\endhead
1 & Support vector machine with fivefold cross validation & & Daily
online stock reviews & SSE 50 & 0.98 \\
1 & Support vector machine with rolling windows & & & & 0.9 \\
1 & Logistic regression with fivefold cross validation & & & & 0.87 \\
2 & Non-ML: & Tokenized and mined using the Harvard General Inquirer
dictionary & Reuters textual data & S\&P 500 & Corr: -0.91 \\
3 & Random forest & Date validation, stop word and URL removal & Tweets
& INDU & 0.98 \\
3 & Decision tree & & & & 0.97 \\
3 & Logistic regression & & & & 0.81 \\
3 & Random forest & & & S\&P 500 & 0.92 \\
3 & Decision tree & & & & 0.88 \\
3 & Logistic regression & & & & 0.77 \\
4 & N-gram, TF-IDF, Naïve Bayes, K-NN & Tokenize, stopwords, stemming,
abbreviation processing & News articles and financial reports & Three
tech stocks & 0.9 \\
5 & TS logistic regression & Feature extraction: lags, window features &
Financial indicators & Individual S\&P 500 stocks & 0.79 \\
5 & TS random forest & & & & 0.78 \\
5 & TS neural network & & & & 0.78 \\
5 & TS gradient boosting & & & & 0.78 \\
6 & Logistic regression & Date validation forward search feature
selection & Market indices, exchange rates & S\&P 500 & 0.61 \\
6 & Gaussian discriminant analysis & & & & 0.61 \\
6 & Naïve Bayes & & & & 0.6 \\
6 & Linear SVM & & & & 0.6 \\
6 & Radial Basis Function SVM & & & & 0.63 \\
6 & Polynomial SVM & & & & 0.6 \\
7 & SVM & Feature extraction and feature selection & Political speeches
and metadata & Liar dataset & 0.74 \\
7 & Multilayer perceptron & & & & 0.55 \\
7 & Convolutional neural network & & & & 0.61 \\
7 & Fasttext & & & & 0.66 \\
7 & BERT & & & & 0.66 \\
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.08}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.76}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.16}}@{}}
\caption{Table 2.1 metadata}\tabularnewline
\toprule
\begin{minipage}[b]{\linewidth}\centering
Paper number
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Title
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Citation
\end{minipage} \\
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{\linewidth}\centering
Paper number
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Title
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Citation
\end{minipage} \\
\midrule
\endhead
1 & Forecasting Stock Market Movement Direction Using Sentiment Analysis
and Support Vector Machine &
(\protect\hyperlink{ref-ren2018forecasting}{Ren \emph{et al.}, 2018}) \\
2 & Extracting News Sentiment and Establishing its Relationship with the
S\&P 500 Index & (\protect\hyperlink{ref-zubair2015extracting}{Zubair \&
Cios, 2015}) \\
3 & An analysis of the impact of President Trump's tweets on the DJIA
and S\&P 500 using machine learning and sentiment analysis &
(\protect\hyperlink{ref-kinyua2021analysis}{Kinyua \emph{et al.},
2021}) \\
4 & Predicting Stock Market Behaviour using Data Mining Technique and
News Sentiment Analysis &
(\protect\hyperlink{ref-khedr2017predicting}{Khedr \emph{et al.},
2017}) \\
5 & Predicting Stock Movement Direction with Machine Learning:an
Extensive Study on S\&P 500 Stocks &
(\protect\hyperlink{ref-jiao2017predicting}{Jiao \& Jakubowicz,
2017}) \\
6 & Forecasting S\&P 500 Stock Index Using Statistical Learning Models &
(\protect\hyperlink{ref-liu2016forecasting}{Liu \emph{et al.}, 2016}) \\
7 & A machine-learning based framework for detection of fake political
speech & (\protect\hyperlink{ref-purevdagva2020machine}{Purevdagva
\emph{et al.}, 2020}) \\
\bottomrule
\end{longtable}

\hypertarget{data-collection-and-cleaning-process}{%
\section{Data collection and cleaning
process}\label{data-collection-and-cleaning-process}}

This section describes the data collection and cleaning process as well
as the feature extraction process. It begins by explaining the three
data-subset collected for this study, namely: control, meta and test
data. Control refers to auto-regressive features extracted from the S\&P
500, meta refers to S\&P 500 adjacent financial data and test refers to
vectorised presidential speeches - which are the focus of this study.
The section begins by elaborating on the collection and cleaning steps
for each of these datasets. The next subsection describes the feature
engineering methods (sentiment analysis and word vectorization) that
were used to extract variables with potentially strong signals from the
text data. The final subsection describes the reasoning, methods and
results used in a time series analysis of the financial time series
(S\&P 500) data in order to extract useable auto-regressive features
from it for the control data-subset.

\hypertarget{data-1}{%
\subsection{Data}\label{data-1}}

Three types of data were gathered for this project, namely: presidential
speeches/text data (all the transcripts from the Presidency Project
website including formal and informal and written and verbal addresses),
a history of the S\&P 500 index and a history of 5 S\&P 500 adjacent
price histories -- i.e.~2 subsets of financial data.

The S\&P 500 index and the metadata was downloaded from Yahoo Finance
while the presidential speeches were scraped from the American
Presidency Project (\protect\hyperlink{ref-yahooSP500}{Yahoo, n.d.})
(\protect\hyperlink{ref-americanPresProj1}{Woolley \& Peters, n.d}). The
S\&P 500 is downloaded using the `fin\_data\_downloader' Python module
and will always download the entire history of the S\&P 500 index at a
daily interval (\protect\hyperlink{ref-rees2022mastersgithub}{Rees,
2022}). The same goes for the metadata. The presidential speeches on the
other hand were scraped using the `WebScrapeAndClean' Python module
which will also always scrape the entire corpus available on the
American Presidency Project website
(\protect\hyperlink{ref-rees2022mastersgithub}{Rees, 2022}). This allows
for perfectly updated data to be collected at any time.

The S\&P 500 data contains seven variables, namely: `Date', `Open',
`High', `Low', `Close', `Adj Close', and `Volume'. `Open' records the
opening price for each day, while `Close' records the closing price.
`High' records the daily high and `Low' the daily low. `Volume' records
the dollar amount of stock traded in the S\&P 500 on each day and `Adj
Close' is irrelevant because it never differs from `Close'. Notably,
opening and closing prices are only differentiated between after April
20th 1982 while volume was only recorded after 1950.

After scraping, the Presidency Project data contains five variables.
These are `Type' which records the category and sub-category of each
speech, `Name' which records the title and name of the main speaker,
`Date' which records the date the speech occurred on, `Title' which
records the title of each speech and `Transcript' which contains the raw
HTML transcript of the speech.

\hypertarget{cleaning}{%
\subsection{Cleaning}\label{cleaning}}

The S\&P 500 index did not require any cleaning after download, besides
the removal of the redundant `Adj Close' variable. Conversely, the
presidential speeches required extensive cleaning. Text that has been
web-scraped contains HTML tags\footnote{HTML tags include paragraphing
  and spacing indicators for computers to interpret such as '}, thus the
first step was to remove the HTML tags from the text. Similarly, the
test contained reactions from the crowds listening to the
speeches\footnote{These were tags such as `Laughter' and `Applause'.},
which were also removed. Next, the transcripts were converted to lower
case and the question sections removed. Next a `No Stops Transcript'
variable was created by removing the stopwords\footnote{Stopwords are
  words that commonly occur in the English language and are therefore
  unlikely to contain any sort of signal and thus constitute only noise.}
in the Natural Language Tool Kit (NLTK) stop words dictionary from the
clean transcript. The original transcripts were also kept. Table A1 in
Appendix A shows the shape of the data at this point. The cleaning of
both the speech data and both financial datasets was done in their
original collection scripts, i.e.~the `fin\_data\_downloader' Python
script and the `WebScrapeAndClean' Python script respectively.

\hypertarget{text-feature-engineering}{%
\subsection{Text feature engineering}\label{text-feature-engineering}}

To increase the strength of the signals coming out of the speech data
and reduce the computational power required to run the machine learning
algorithms -- feature engineering is required. Feature engineering
refers to the emphasis of certain signals within the available data and
creation of new variables which capture these signals. It results in the
addition of extra features (variables) to the dataset. There are three
broad methods of feature engineering: feature selection, feature
extraction and the creation of new features
(\protect\hyperlink{ref-geron2019hands}{Géron, 2019: 27}). In this
section the creation of new features occurred and took the form of
sentiment analysis and word vectorization for the text data.

\hypertarget{sentiment-analysis}{%
\subsubsection{Sentiment analysis}\label{sentiment-analysis}}

Two versions of sentiment analysis were carried out. First, NLTK's
Valence Aware Dictionary for Sentiment Reasoning (VADER) was used to
extract a sentiment analysis tuple, in this instance containing four
scores, namely, negativity, neutrality, positivity and compound. VADER
is a lexicon based system of sentiment analysis
(\protect\hyperlink{ref-sohangir2018financial}{Sohangir, Petty \& Wang,
2018}). Each of negativity, neutrality and positivity describe a
transcript independently of the other scores while compound describes a
transcript comprehensively (combining the other three scores). VADER,
when compared with alternative NLP feature extraction techniques has
performed better on social media transcripts and generalized better to
other areas \protect\hyperlink{ref-elbagir2019twitter}{Elbagir \& Yang}
(\protect\hyperlink{ref-elbagir2019twitter}{2019}). VADER has been used
in vectorizing text relative to financial data in
\protect\hyperlink{ref-pano2020complete}{Pano \& Kashef}
(\protect\hyperlink{ref-pano2020complete}{2020}) for Bitcoin price
predictions, \protect\hyperlink{ref-agarwal2020sentiment}{Agarwal}
(\protect\hyperlink{ref-agarwal2020sentiment}{2020}) which found a
strong correlation between VADER sentiment scores and stock price
changes and \protect\hyperlink{ref-sohangir2018financial}{Sohangir
\emph{et al.}} (\protect\hyperlink{ref-sohangir2018financial}{2018})
which shows the superiority of lexicon based approaches (specifically
VADER) over ML approaches for sentiment classification.

Next, the TextBlob package's sentiment analysis tool was used. This
yields two scores (in a tuple) describing the sentiment of a transcript,
namely, a polarity score (ranging from -1 to 1) and a subjectivity score
(ranging from 0 to 1). Polarity describes whether the emotions expressed
are negative or positive, with lower scores indicating negativity while
subjectivity indicates the extent of the usage of subjective words
(\protect\hyperlink{ref-loria2018textblob}{Loria \& others, 2018}).
\protect\hyperlink{ref-biswas2020examining}{Biswas, Sarkar, Das, Bose \&
Roy} (\protect\hyperlink{ref-biswas2020examining}{2020}) use Textblob
sentiment scores in their analysis of the effects of Covid-19 on stock
markets. Textblob was also tested in
\protect\hyperlink{ref-sohangir2018financial}{Sohangir \emph{et al.}}
(\protect\hyperlink{ref-sohangir2018financial}{2018}) but did not
perform as well as VADER although it did outperform ML methods in terms
of area under the curve (AUC) scores. It is expected that the VADER
sentiment tuple will outperform Textblob's sentiment tuple as a
predictor of the S\&P 500 data.

\hypertarget{speech-vectorization}{%
\subsubsection{Speech vectorization}\label{speech-vectorization}}

Converting human readable text into machine readable data requires the
conversion from words to numbers. This vectorization can be done in
various ways but in order to preserve the meaning of the texts the
Word2Vec and Doc2Vec Python packages provided by Gensim were used
(\protect\hyperlink{ref-rehurek2011gensim}{Rehurek \& Sojka, 2011}).
However, Word2Vec was originally published in two papers by
\protect\hyperlink{ref-mikolov2013efficient}{Mikolov, Chen, Corrado \&
Dean} (\protect\hyperlink{ref-mikolov2013efficient}{2013}) and
\protect\hyperlink{ref-mikolov2013distributed}{Mikolov, Sutskever, Chen,
Corrado \& Dean} (\protect\hyperlink{ref-mikolov2013distributed}{2013})
while Doc2Vec was suggested by
\protect\hyperlink{ref-le2014distributed}{Le \& Mikolov}
(\protect\hyperlink{ref-le2014distributed}{2014}).

\hypertarget{word2vec}{%
\paragraph{Word2Vec}\label{word2vec}}

Gensim's Word2Vec Python package's skip-gram model is used for Word2Vec
vectorization. Using the full vocabulary of words in a corpus of
speeches and one-hot encoding for word vectors, Word2Vec trains a single
hidden layer neural network to predict words based on the words around
them. The parameters used for training are available in Appendix A.

The model contextually embeds each word in the entire corpus of speeches
by running the speeches through the single hidden layer predictive
neural network (NN). The NN is provided with the pseudo-task of
predicting the word of focus from the words surrounding it each time it
occurs in the corpus. Thus, a hidden layer is trained to contain the
information that contextually embeds each word in the corpus. These
hidden layer vectors (rather than the predictions) are the real output
of the Word2Vec model. Words that appear in similar contexts throughout
the corpus will have similar representational vectors (hidden layers)
and thus can be said to have similar meanings in the corpus
\protect\hyperlink{ref-mikolov2013distributed}{Mikolov \emph{et al.}}
(\protect\hyperlink{ref-mikolov2013distributed}{2013}).

An example phrase might be `The quick brown fox jumps'. If the word
`brown' is the focus word, the words `quick' and `fox' would be fed into
the neural network which would then be trained to map the input to the
word `brown'. Doing this for every instance of `brown' in a corpus
creates a hidden layer that contains all the contextual information
required to predict the word `brown' in the given corpus. Figure 1
depicts this process.

The model is extremely good at relating words that appear in similar
contexts to each other. For example, when asked for the three words most
similar to `oil' the model trained on the presidential speeches corpus
returns `crude', `gas' and `petroleum'. The input `gold' returns
`silver', `bullion' and `coin'; whilst `virus' returns `covid19',
`infection' and `pandemic'. In this study the representational vectors
each contain 200 elements (because the hidden layers were set to contain
200 elements). Thus each word is described by a vector containing 200
elements. In order to create a similarly sized vector for each speech,
the vectors describing all the words in each speech were averaged. Thus
each speech has been reduced to a 200 element vector averaging the
contextual embeddings of each word contained therein. This averaging
technique was also used in
\protect\hyperlink{ref-vargas2017deep}{Vargas, De Lima \& Evsukoff}
(\protect\hyperlink{ref-vargas2017deep}{2017}) and
\protect\hyperlink{ref-qin230natural}{Qin}
(\protect\hyperlink{ref-qin230natural}{2018}). However, this method
fails to preserve word order in a document vector
(\protect\hyperlink{ref-le2014distributed}{Le \& Mikolov, 2014}).

Notably, the algorithm also makes room for two-word phrases such as
`asset backed' or `short selling' -- which are considered as
`assetbacked' and `shortselling'. When two words that occur irregularly
in the vocabulary occur together frequently the algorithm interprets
them as a phrase and treats them as such. There is, however, only room
for two words in each phrase if the algorithm has only been executed
once -- which is the case here.

Word2vec is used by (\protect\hyperlink{ref-shi2019word2vec}{Shi, Zhao
\& Xu, 2019}) for the improvement of sentiment classification which
implies that the final vectors in this study may contain sentiment
information. It is also used by
\protect\hyperlink{ref-vargas2017deep}{Vargas \emph{et al.}}
(\protect\hyperlink{ref-vargas2017deep}{2017}) and
\protect\hyperlink{ref-qin230natural}{Qin}
(\protect\hyperlink{ref-qin230natural}{2018}) in their predictive
modelling of S\&P 500 changes based on Twitter data.
\protect\hyperlink{ref-vargas2017deep}{Vargas \emph{et al.}}
(\protect\hyperlink{ref-vargas2017deep}{2017}) also used 200 element
vectors while \protect\hyperlink{ref-qin230natural}{Qin}
(\protect\hyperlink{ref-qin230natural}{2018}) used 300 element vectors.
Both studies achieved prediction accuracy around 65\%.

\begin{figure}
\centering
\includegraphics{Images/Word2Vec_model.png}
\caption{Word2Vec model}
\end{figure}

\hypertarget{doc2vec}{%
\paragraph{\texorpdfstring{Doc2Vec
\label{Doc2Vec}}{Doc2Vec }}\label{doc2vec}}

An alternative method to creating a vector representation of a sentence
or document is Doc2Vec. This method is similar to the Word2Vec method
described above but includes an additional floating vector when
performing its pseudo-task. This vector is maintained across every word
prediction task within a document and subjected to training for each
instance of every word. Thus each document in a corpus is assigned a
single comprehensive vector that embeds it in the corpus. Embedding
within the broader corpus is maintained by tagging each document with a
unique tagging phrase. This method outperforms other methods such as
bag-of-words for text representations
(\protect\hyperlink{ref-le2014distributed}{Le \& Mikolov, 2014}).

There are two implementations of Doc2Vec, namely Distributed Bag of
Words (DBOW) and Distributed Memory (DM)
(\protect\hyperlink{ref-sohangir2018financial}{Sohangir \emph{et al.},
2018}). Both have been used in this study. In both cases each document
is assigned a paragraph tag which represents the paragraph to the
Doc2Vec algorithm. DM Doc2vec does this in the same way that a word
represents itself to the Word2Vec algorithm. For every word in a
document, its paragraph tag is passed to the Doc2Vec algorithm along
with the words relevant to the current prediction pseudo-task. Back
propagation is employed in the same manner as in Word2Vec except that
the document tag vector is optimized for separately from the word
vectors. This document tag vector is the relevant output in this case.
Because a single vector of weights is created for each document, this
vector should constitute a vectorized representation of the document as
a whole. Figure 2 depicts the architecture of DM Doc2Vec algorithms.
Alternatively, DBOW Doc2Vec algorithms ignore the context of a word and
use random sampling to predict words from a paragraph. Figure 3 depicts
the architecture of a DBOW Doc2Vec algorithm.

\begin{figure}
\centering
\includegraphics{Images/Doc2Vec_distributed_memory.png}
\caption{Doc2Vec model - distributed memory architecture: dm = 1}
\end{figure}

\begin{figure}
\centering
\includegraphics{Images/Doc2Vec_distributed_bow.png}
\caption{Doc2Vec model - distributed bag of words architecture: dm = 0}
\end{figure}

!!!!!!!!WHERE HAS DOC2VEC BEEN USEFUL FOR FINANCIAL
PREDICTIONS!!!!!!!!!!

\hypertarget{sp-500-time-series-analysis}{%
\subsection{S\&P 500 time series
analysis}\label{sp-500-time-series-analysis}}

As part of feature extraction - econometric time series analysis has
been run on the S\&P 500 data. The aim of this analysis was to find the
linear model of best fit to the S\&P 500 data and then include relevant
auto-regressive variables in the final dataset under the assumption that
they will be relevant in the highly non-linear ML models. An initial
analysis was done on the S\&P 500 data after April 20th 1982 because
opening and closing prices are differentiated between from that date
onwards. This analysis found a large array of significant variables --
more than half the days of the month, the month of September, a few
specific years, 10 non-consecutive lags and the first lag of volume of
trade. The significance of these variables, particularly the days of the
month were difficult to explain rationally. However, they did indicate
persistence of volatility. This volatility persistence and the fact that
volume is a strong indicator of absolute price change encouraged a
second round of analysis that was done on all data following the initial
recording of volume in 1950. This second round of analysis was focussed
on finding an auto-regressive conditional heteroskedasticity (ARCH)
model that fit the data well. The analysis concluded with the selection
of an ARMA (1,0) GARCH (1,1) model. Thus, the core variables selected
for inclusion in the final data set are the first lag of standardized
volume, the first lag of daily percentage change, the first residual of
daily percentage change (DPC) predicted by an ARMA (1,0) model and the
first lag of variance of the DPC.

\hypertarget{onwards}{%
\subsubsection{1982 onwards}\label{onwards}}

Running time series analysis on the daily percentage change in the S\&P
500 after April 20th 1982 has revealed 10 significant non-consecutive
lags. The `Daily percentage change `variable was created by taking the
percentage difference between the `Close' and `Open' variables for each
day of the data. Before April 20th 1982 the `Open' and `Close' variables
hold identical values, hence the time series analysis only being run
after that date. As can be seen in Figure 1, `Daily percentage change'
is naturally stationary. This is supported by the results of an
Augmented Dickey-Fuller unit root test which indicated that no unit root
is present in the data. Running a partial autocorrelation function
revealed 10 significant lags (these were lagged by 1, 2, 4, 12, 15, 16,
18, 27, 32, and 34 periods). Regressing `Daily percentage change' on all
10 significant lags reinforced the finding by yielding significance
above the 95\% confidence interval for all 10 lags. Further, regressing
the daily percentage change on weekday, monthday, month and year
categorical variables yielded the significant correlations depicted in
Table 1 (none of the weekdays were significant). Regressing on the
categorical variables and the lags simultaneously yielded similar
results with increased significance for 2002 (to the 99\% level) and
2008 (to the 99,9\% level) and the addition of 2018 (significant at the
95\% level), further an extra 4 monthdays were deemed significant -
bringing the total to 21 (out of 31) significant monthdays, finally,
some of the significance levels on the lags changed. These statistics
are depicted in Table 2. Adding a normalized (distributed standard
normal) volume variable lagged by one period to the regression yields a
significance at the 99\% level on the lagged volume variable and alters
the significance on the year variables as reported in Table 3.

While the years 2002 and 2008 are justifiable as significant because of
the financial crashes that happened in each of those years (2002 -- dot
com bubble and 2008 housing bubble) and September is also relatable to
the housing bubble; it is difficult to rationally justify the monthday
variables as significant regardless of their quantitative significance.
The high number of lags is also difficult to justify and implies rather
that there may be persistence of volatility. Thus the following section
focusses on the modelling of volatility over a time period that
maximises the inclusion of volume statistics in the data. All of the
analysis reported in this section was done in R using the packages
`stats', `dplyr', `urca', `tidyverse', `ggplot2' and `fixest' (Marais,
2022).

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{Exploratory Regression Output 1}
 \label{Reg01}
\setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont Reg1} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont Reg2} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (Intercept)} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.032 ***} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.031 ***} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_2} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.020 **\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_6} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.022 **\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_8} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.015 *\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_12} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.031 ***} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.031 ***} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_15} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.025 ***} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.024 **\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_16} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.040 ***} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.040 ***} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_18} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.018 *\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_26} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.026 ***} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.025 ***} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_27} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.022 **\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_29} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.018 *\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont LD\_34} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.033 ***} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.034 ***} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.007)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont N} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 18138\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 18138\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont R2} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.007\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.005\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05.} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{Exploratory Regression Output 2}
 \label{Reg01}
\setlength{\tabcolsep}{0pt}
\begin{tabular}{l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont Reg3} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (Intercept)} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.059 ***} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.008)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont September::1} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.071 **\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.027)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont Monday::1} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont -0.116 ***} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont } \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont (0.019)\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont N} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 18172\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont R2} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont 0.002\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{2}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} {\fontsize{12pt}{14.4pt}\selectfont  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05.} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{onwards-arch-model}{%
\subsubsection{1950 onwards -- ARCH model}\label{onwards-arch-model}}

A second round of time series analysis was performed on all the data
after Jan 1st 1950 which revealed that an ARMA(1,0) GARCH(1,1) model
might be the most appropriate fit to the data and that the first lag on
a date controlled version of standardized volume is a good predictor of
DPC. The secondary analysis was performed because the number (and
non-consecutiveness) of significant lags in the first model was out of
the ordinary. The choice was made to extend the dataset back to 1950
because lagged volume appeared to be significant and volume data is
available from that time onwards.

The extended dataset required a different dependent variable because
open and closing prices were only differentiated between after April
20th 1982. Thus, the inter-day percentage change in closing price
constituted the dependent variable in the second round of analysis.
Figure 4 depicts the Daily Percentage Change (DPC) in the S\&P 500 from
1950 until present -- the persistence of volatility is again apparent.
Figure 5 illustrates the correlation between the DPC and standardized
volume which justifies the extension of the dataset.

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/GSPC_figure_4-1} 

}

\caption{S and P 500 Daily Percentage Change 1950 – Present \label{Figure4}}\label{fig:GSPC_figure_4}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/GSPC_figure_5-1} 

}

\caption{Standardized volume (red) and absolute logDiff (green) \label{Figure5}}\label{fig:GSPC_figure_5}
\end{figure}

\hypertarget{volume}{%
\paragraph{Volume}\label{volume}}

Regressing the DPC on the first lag of standardized volume yielded
insignificant results. However, regressing absolute DPC
(pseudo-volatility) on the first lag of standardized volume (stdVol\_1)
yielded a coefficient of 0,148 significant above the 99,9th percentile.
Figure 6 shows this relationship. Figure 6 also shows that standardized
volume (stdVol) trends upwards over time. Further, regressing stdVol\_1
on Date (regression 6) yields a positive coefficient significant above
the 99,9th percentile and an adjusted R2 of 0,6. This shows that a large
percentage of the variation in stdVol is explained by variation in Date.
Thus to avoid implicitly including a Date proxy\footnote{The goal of
  this project is to create a model that can predict future S\&P 500
  returns. Thus, to include a proxy for date (in this case in the date
  trending stdVol) would cause data contamination since dates do not
  hold any information relevant to the S\&P 500 returns besides price or
  volatility trends.} as a predictor of S\&P 500 returns the included
volume variable must be adjusted to be stationary over time. The
residuals from regression 6 make up the variation in stdVol that cannot
be explained by the variation in Date -- thus they are a Date neutral
measure of stdVol. Figure 7 shows the difference between stdVol and
stdVol controlled for variance due to Date (stdVolControlled).
Regressing the residuals of regression 6 on Date (regression 7) shows no
significant relationship between the two while regressing absolute DPC
on the residuals from regression 6 (regression 8) yields a coefficient
of 0,164 significant above the 99,9th percentile. Figure 8 shows the
relationship between absolute DPC and stdVolControlled. Thus these
residuals (rather than stdVol) should be included as a variable in the
final prediction data.

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/GSPC_figure_6-1} 

}

\caption{Absolute logDiff and stdVol \label{Figure6}}\label{fig:GSPC_figure_6}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/GSPC_figure_7-1} 

}

\caption{DateResid of StdVol (Red) and stdVol (Green) \label{Figure7}}\label{fig:GSPC_figure_7}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/GSPC_figure_8-1} 

}

\caption{stdVolControlled and absolute LogDiff \label{Figure8}}\label{fig:GSPC_figure_8}
\end{figure}

\hypertarget{arch-and-garch-testing}{%
\paragraph{\texorpdfstring{ARCH and GARCH testing
\label{ARCHandGARCH}}{ARCH and GARCH testing }}\label{arch-and-garch-testing}}

The LJung-Box test is a standard inference test in time series analysis.
It tests the null-hypothesis that a series is a white noise series
(\protect\hyperlink{ref-hassani2020selecting}{Hassani \& Yeganegi,
2020}). Selection of the number of lags to be included in the Ljung-Box
test is discussed by
\protect\hyperlink{ref-hassani2020selecting}{Hassani \& Yeganegi}
(\protect\hyperlink{ref-hassani2020selecting}{2020}). Critical to this
decision is the ratio of lags (H) to the number of observations (T) in
the data set. If the size of H is too large the actual size of the test
exceeds the nominal size of the test and the integrity of the test is
violated. The dataset for this set of tests contains 18143 observations.
The smallest (most difficult to satisfy) ratio permitted in the
discussion is 0,001 suggested by
\protect\hyperlink{ref-hyndman2018forecasting}{Hyndman \&
Athanasopoulos} (\protect\hyperlink{ref-hyndman2018forecasting}{2018}).
Thus the maximum number of lags that satisfy the smallest H/T ratio is
18 and any number of lags less than 18 is permissible.

Running a 10-Lag LJung Box test on the DPC yields a p-value of 0.0002
which indicates that the null hypothesis should be rejected and that the
series is not a white noise series. I.e. ARCH effects are present in the
series. Running autocorrelation and partial-autocorrelation tests on the
absolute DPC indicates that there is a degree of persistence of
volatility (\protect\hyperlink{ref-kotze2021volatility}{Kotzé, 2021}).
This can be seen in Figure 9. A t-test indicates that the mean of the
population is significantly different from zero (0.036). Thus, demeaning
the equation constitutes the construction of a mean equation
(essentially the demeaned DPC) represented in the series -- a demeaned
daily percentage change(DDPC)
(\protect\hyperlink{ref-kotze2020univariate}{Kotzé, 2020}). A 10 lag
Ljung-Box test again indicates the presence of ARCH effects specifically
in the 1st,2nd, 5th, 6th, 8th, and 9th lags. However, running a multiple
linear regression (OLS) of DDPC on its lags reveals only the 2nd, 6th,
8th and 9th lags as significant above the 95th percentile.

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/ARCH_figure_9-1} 

}

\caption{Autocorrelation and partial-autocorrelation functions of absolute LogDiff \label{Figure9}}\label{fig:ARCH_figure_9}
\end{figure}

Eight models were tested for performance considering the above results.
Model 1 is an ARIMA(0,0,1), model 2 is an ARIMA(1,0,0), model 3 is an
ARMA(1,0) GARCH(1,1), model 4 is a GARCH(1,1), model 5 is an
EGARCH(1,1), model 6 is an ARMA(1,0) ARCH(1,0), model 7 is an ARCH(1,0)
and model 8 is an ARMA(2,0) GARCH(1,1). The inclusion of an EGARCH model
is motivated by its use in Nelson (1991), Blazsek \& Mendoza (2016) and
Tiwari, Raheem \& Kang (2019) in their time series analyses of the S\&P
500. Table 4 compares the coefficients from the 6 ARCH model variations
(models 3, 4, 5, 6, 7 and 8). Figure 10, 11 and 12, 13, 14 and 15 show
the results of autocorrelation and partial-autocorrelation functions of
the residuals of the 6 ARCH variation models. Notably, all three of the
models that do not contain an ARMA(1,0) component (models 4,5 and 7)
yield a significant first residual indicating that inclusion of the
ARMA(1,0) component is important. However, all the models containing an
ARCH(1,0) - besides model 6 - component yield a significant second
residual Further, model 6 -- the non-generalized ARCH model -- has
intermittent significant residuals indicating that volatility is
clustered and persistent. Persistent volatility is one of the oversights
of the ARCH model that is addressed in the GARCH model. As can be seen,
Model 8 (Figure 3.15) yields the least significant residuals and as such
has been selected as the model that best fits this time series. The
model linearly predicts the dependent variable using the first and
second lags of the dependent variable, the first residual of an
ARMA(1,0) model and the first lag of variance in the time series. As
such the 1st and 2nd lags of DlogDif and the first lag of variance of
logDif will be included in the final dataset
(\protect\hyperlink{ref-kotze2020univariate}{Kotzé, 2020}).
Additionally, as illustrated in Aras (2021) the inclusion of the GARCH
model prediction can be beneficial to ML model training, thus the
predictions of model 8 will also be included in the dataset.

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/ARCH_figure_10-1} 

}

\caption{Autocorrelation and partial-autocorrelation functions of model 3 - ARMA(1,0) GARCH(1,1) - residuals \label{Figure9}}\label{fig:ARCH_figure_10}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/ARCH_figure_11-1} 

}

\caption{Autocorrelation and partial-autocorrelation functions of model 4 - GARCH(1,1) -  residuals \label{Figure9}}\label{fig:ARCH_figure_11}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/ARCH_figure_12-1} 

}

\caption{Autocorrelation and partial-autocorrelation functions of model 5 - EGARCH(1,1) -  residuals \label{Figure9}}\label{fig:ARCH_figure_12}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/ARCH_figure_13-1} 

}

\caption{Autocorrelation and partial-autocorrelation functions of model 6 - ARMA(1,0) ARCH(1,0) - residuals \label{Figure9}}\label{fig:ARCH_figure_13}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/ARCH_figure_14-1} 

}

\caption{Autocorrelation and partial-autocorrelation functions of model 7 - ARCH(1,0) - residuals \label{Figure9}}\label{fig:ARCH_figure_14}
\end{figure}

\begin{figure}[H]

{\centering \includegraphics{Thesis_WriteUp_files/figure-latex/ARCH_figure_15-1} 

}

\caption{Autocorrelation and partial-autocorrelation functions of model 8 - ARMA(2,0) GARCH(1,1) - residuals \label{Figure9}}\label{fig:ARCH_figure_15}
\end{figure}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.14}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.16}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.11}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.13}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.19}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.11}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.16}}@{}}
\caption{Significancies of various ARCH models}\tabularnewline
\toprule
\begin{minipage}[b]{\linewidth}\centering
Statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model3
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model4
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model5
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model6
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model7
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model8
\end{minipage} \\
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{\linewidth}\centering
Statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model3
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model4
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model5
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model6
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model7
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Model8
\end{minipage} \\
\midrule
\endhead
Specification & ARMA(1,0) G(1,1) & G(1,1) & EG(1,1) & ARMA(1,0)
ARCH(1,0) & ARCH(1,0) & ARMA(2,0) G(1,1) \\
\(\mu\) & 1 & 1 & 1 & 1 & 1 & 1 \\
AR1 & - & \textless2e-16 *** & 6.16e-08 *** & - & - & \textless{} 2e-16
*** \\
AR2 & - & - & - & - & - & 0.00355 ** \\
\(\omega\) & 2.22e-16 *** & \textless2e-16 *** & \textless2e-16 *** &
\textless2e-16 *** & \textless0.005 *** & \textless2e-16 *** \\
\(\alpha\) 1 & \textless2e-16 *** & \textless2e-16 *** & \textless2e-16
*** & \textless2e-16 *** & \textless2e-16 *** & \textless2e-16 *** \\
\(\beta\) 1 & \textless2e-16 *** & \textless2e-16 *** & - & - &
\textless2e-16 *** & \textless2e-16 *** \\
\(\gamma\) 1 & - & - & - & - & \textless2e-16 *** & - \\
\bottomrule
\end{longtable}

\hypertarget{additional-variables}{%
\subsection{\texorpdfstring{Additional variables
\label{addVars}}{Additional variables }}\label{additional-variables}}

Four additional auto-regressive variables were included to bolster the
information available to the algorithms under the assumption that the
linear modelling completed here is insufficient to exhaustively extract
useful variables. To account for persistence in positive or negative
momentum, such as in an EGARCH model, a positive and negative transform
variable was created. The mean of the positive entries in the DlogDif
variable was juxtaposed with the mean of the negative entries to create
a ratio of 1:-1.05 for positive to negative entries. Thus, where DlogDif
is negative posNegTransform is -1.05 and where posNegTransform is
positive posNegTransform is 1. Another three variables were added to
account for persistence in extreme price movements such as occured in
the 2002 or 2008 financial crises. These are blackSwan\_SD3,
blackSwan\_SD4 and blackSwan\_SD5. They are dummy variables set to 1
where the DlogDif variable lies outside 3, 4 or 5 standard deviations
from the mean of the series, respectively. Notably, all 3 black swan
variables and the posNegTransform are lagged by one period. Thus another
4 auto-regressive variables are added to the dataset.

Further, five S\&P 500 adjacent indicators were selected for this study.
Notably they were only included in the speech-centred dataset. The five
series included were the NASDAQ composite index, the oil price, the
Shanghai Stock Exchange composite index (SSE), the Dollar strength index
and the CBOE volatility index (VIX).

\hypertarget{summary-of-data-collection-and-preparation}{%
\subsection{Summary of data collection and
preparation}\label{summary-of-data-collection-and-preparation}}

The data preparation phase of this project consisted of four sections.
These were, data collection, data cleaning, text data feature
engineering and financial data feature engineering. Data collection and
cleaning consisted of web scraping, removing HTML tags, converting to
lowercase, stopword removal and lemmatization for the text data.
Collection of the financial data is a done via a Python script that will
always download the entire history of the S\&P 500. Cleaning requires
only the removal of the adjusted close variable. Text data feature
engineering took the form of Textblob sentiment analysis, VADER
sentiment analysis, Word2Vec vectorization and Doc2Vec vectorization.
The results of which were 2 sentiment descriptors from Textblob, 4
sentiment descriptors from VADER, a 200 element average descriptor
vector from Word2Vec, a 200 and a 20 element vector speech descriptor
from Doc2Vec for the speech-centred data, and two 21 element vector
speech descriptors from Doc2Vec for the date-centred data for a total of
426 variables (features) engineered from the text data for the
speech-centred data and 42 variables engineered from the text data for
the date-centred data. The financial data analysis took the form of time
series econometrics. This analysis resulted in the fitting of an
ARMA(2,0) GARCH(1,1) model to the engineered daily percentage change
variable. The model linearly predicts the dependent variable using the
first and second lags of the dependent variable, the first residual and
the first lag of variance in the time series. Thus the first and second
lags of the log difference of the closing price and the first lag of
variance of log difference are included in the data. Additionally, the
prediction of the ARMA(2,0) GARCH(1,1) model and the first lag of
standardized volume of trade are included. Thus 4 variables (features)
have been engineered using time series analysis of the S\&P 500 data.
Additionally, a further four auto-regressive variables were added to
make up for the flaws inherent in purely linear modelling and six S\&P
500 adjacent variables were also selected for addition to the dataset.

\hypertarget{experiment-design}{%
\section{Experiment design}\label{experiment-design}}

Two sets of experiments were run on two categories of data design. The
first is a speech-centred design. For every speech in this dataset there
is a row of data attached. Thus there is no data for dates when no
speeches occurred and there are duplicate dates because there are days
when more than one speech occurred. This dataset is the larger of the
two datasets and contains 35 251 rows of data -- one for each unique
speech since 1998-01-01.

Based on the assumption that every speech that occurs on a day affects
the closing price of the S\&P 500 on that day - the second dataset uses
a date-centred design. For every date (trading day) there is one row in
the dataset. Thus on dates when more than one speech occurred the speech
data has been aggregated into a single vector (see the Doc2Vec methods
section for an explanation of this method). The date-centred dataset
contains 25 383 rows of data -- one for each trading day between
1950-01-01 and 2022-03-22. The speech-centred and date-centred datasets
are exemplified in Table 5 and 6, respectively.

\begin{longtable}[]{@{}cccccc@{}}
\caption{Example of the speech-centred data design -- Note that there is
no data for dates that no speeches occurred and that there are duplicate
dates. Note also that on dates where more than one speech occurred there
is duplicate data in the LD\_Date\_Resid\_1 column which is
representative of the auto-regressive data in this example. Finally,
note that the V1 and V2 columns (representative of the vectorized speech
data) is different for every row of data.}\tabularnewline
\toprule
Date & Speech & LD\_Date\_Resid\_1 & V1 & V2 & V3 \\
\midrule
\endfirsthead
\toprule
Date & Speech & LD\_Date\_Resid\_1 & V1 & V2 & V3 \\
\midrule
\endhead
1950-01-02 & Good morning\ldots{} & 0.02 & 3.6 & 2.6 & 1 \\
1950-01-06 & Ladies and\ldots{} & -0.05 & 2.1 & 2 & 0.36 \\
1950-01-06 & Congress\ldots{} & -0.05 & 0.12 & 3.4 & 2.8 \\
\ldots{} & & & & & \\
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}cccccc@{}}
\caption{Example of the date-centred data design -- Note that every
consecutive trading day is included in the data. Note also that on days
when no speeches occurred the V1 and V2 (representing vectorized speech
data) from the last date that a speech occurred are duplicated and the
number of days since the last speech is recorded. Finally, note that on
days when multiple speeches occurred there is only one row of data --
i.e.~the speeches are amalgamated into a single vector.}\tabularnewline
\toprule
Date & Speech & LD\_Date\_Resid\_1 & Days since last speech & V1 & V2 \\
\midrule
\endfirsthead
\toprule
Date & Speech & LD\_Date\_Resid\_1 & Days since last speech & V1 & V2 \\
\midrule
\endhead
1950-01-02 & Good morning\ldots{} & 0.02 & 0 & 1.2 & 1.8 \\
1950-01-03 & N/A & 0.025 & 1 & 1.2 & 1.8 \\
1950-01-04 & N/A & -0.03 & 1 & 1.2 & 1.8 \\
1950-01-05 & (Multiple speeches) & 0.05 & 0 & 2.1 & -0.6 \\
1950-01-06 & Today marks\ldots{} & -0.03 & 0 & 5.3 & -0.86 \\
1950-01-09 & N/A & 0.04 & 3 & 5.3 & -0.86 \\
\ldots{} & & & & & \\
\bottomrule
\end{longtable}

For the speech-centred dataset a total of 1152 models were tested. These
consisted of 384 regression tasks and 768 classification tasks. While
for the date-centred data 336 models were tested. These consisted of 112
regression models and 224 classification models. The models were
selected by incrementally altering a single hyperparameter across 5
fields for regression and 6 fields for classification. These fields are
detailed in the following sections.

\hypertarget{regression-fields}{%
\subsection{Regression fields}\label{regression-fields}}

The 5 fields of hyperparameters for regression were StartDates,
Remove\_duplicates, Reg\_Types, Reg\_algos and Datasets. StartDates
refers to the date from which the dataset began: 1998-01-01, 2000-01-01
and 2010-01-01. The 1998 dataset runs from 1998-01-01 until 2022-03-22,
the 2000 dataset runs from 2000-01-01 until 2022-03-22 etc.

The Remove\_duplicates field divides the speech-centred data into two
subsets -- one with all the duplicate dates removed and one including
the duplicate dates. Duplicate dates occurred because some speeches
occurred on the same date. This hyperparameter is necessary because the
Meta and Auto datasets only contain one value in each field -- thus
duplicating dates duplicates Meta and Auto data which creates the risk
of data contamination between the train and test sets. This field is not
valid for the dates centred dataset and is set to `False' for all the
models using it.

The Reg\_Types field contains two options, TS\_Regressor and
CS\_Regressor. These refer to the manner in which the data is split for
cross validation. The TS\_Regressor cross validation method splits the
data into 5 time-consecutive subsets using the `TimeSeriesSplit' out of
sample (OOS) method from Sci Kit Learn. Model training is done on the
first subset, then the first and second subset etc. up until the fourth
subset. Training scores are calculated for each of the four training
subsets and the maximum score is passed forward as the final
representation of the training score. Test scores are calculated for
only the fifth (and latest) split. This method is employed to force the
algorithm to perform prediction instead of interpolation. It avoids
giving the models access to the future of a data trend when making
predictions for a data point. However, the method is not strictly
necessary in this case because the dependent variable has been centred
and is close to normally distributed -- i.e.~it does not have a trend
(Bergmeir, Hyndman \& Koo, 2018). The CS\_Regressor option performs
regular 5-fold cross validation.

The Reg\_algos field refers to the four different ML algorithms
available for training. These are stochastic gradient descent, a 3
hidden layer neural network, multiple linear regression and gradient
boosting. The hyper-parameters for each of these fields can be found in
Appendix B.1.

Datasets is a complicated field. There are three groups of prediction
sub-datasets in the speech-centred dataset -- the control subset:
X\_control ; the meta subset: X\_meta ; and the test subset: X\_test.
X\_control contains the auto-regressive variables described in section
\ref{ARCHandGARCH} and \ref{addVars}, X\_meta contains the meta
variables (also described in section \ref{addVars}) and X\_test contains
the variables of focus -- the NLP variables. However, in total there are
458 variables across these three subsets. The X\_test subset makes up
the majority of this. There are 4 variables derived from the VADER
sentiment analysis, 2 variables from the textBlob sentiment analysis,
200 variables from Word2Vec, 200 variables from Doc2Vec\_200 and 20
variables from Doc2Vec\_20.

In order to minimize training times and redundancy the X\_test dataset
was reduced to contain 26 variables. These are the VADER and textBlob
sentiment scores and the Doc2Vec\_20 word embeddings. The Word2Vec
embeddings were dropped after underperforming in a series of prediction
tasks when compared with the Doc2Vec embeddings. The Doc2Vec\_20 and
Doc2Vec\_200 embeddings performed similarly on the prediction tasks and
combining them did not improve performance. Thus, to minimize the number
of variables the ML-models had to contend with, the Doc2Vec\_20
embeddings were kept.

The date-centred dataset also contains three groups of prediction
subsets. These are the same X\_control dataset as in the speech-centred
data, and two versions of the vectorised speech data -- the distributed
bag of words (DBOW) subset and the distributed memory (DM) subset --
both detailed in section \ref{Doc2Vec}. A detailed description of the
variables available and tested is available in Appendix B.3.

All 8 X\_control variables were included in both the speech-centred and
the date-centred datasets. The final X\_meta variables selected were
Nasdaq\_ld\_1, Oil\_ld\_1, SSE\_ld\_1, USDX\_ld\_1 and VIX\_ld\_1. The
final X\_test variables selected were the VADER scores, the TextBlob
scores and the set of 20 Doc2Vec variables. While the DBOW and DM
datasets each contained all 20 of their variables and the number of days
since the last speech. The datasets field provided the option for any
combination of each data designs' three subsets as training variables
for a sum of 7 combinations each. Additionally a PossibleBest set of
variables was selected for the speech-centred dataset using an elastic
net regression algorithm from SciKitLearn (hyperparameters available in
appendix B). It was decided to create a 10 variable dataset using 4 NLP
variables, 3 control variables and three meta variable. The elastic net
selected DlogDif\_1, DlogDif\_2, pos\_neg\_transform, Nasdaq\_ld\_1,
Oil\_ld\_1, VIX\_ld\_1, DV\_20\_6, DV\_20\_8, DV\_20\_13, DV\_20\_15 as
the most prominent variables in each of the three subsets of variables.
The elastic net regressions are depicted in Figures 4.1, 4.2 and 4.3.

\begin{figure}
\centering
\includegraphics{Images/ElasticGrid_control_1_0,1.png}
\caption{Elastic net: test set}
\end{figure}

\begin{figure}
\centering
\includegraphics{Images/ElasticGrid_meta_2_0,2.png}
\caption{Elastic net: meta set}
\end{figure}

\begin{figure}
\centering
\includegraphics{Images/ElasticGrid_DV_20_3_0,1.png}
\caption{Elastic net: control set}
\end{figure}

\hypertarget{classification-fields}{%
\subsection{Classification fields}\label{classification-fields}}

The 5 fields of hyperparameters for classification were StartDates,
Remove\_duplicates, Binary, Clf\_Types, Clf\_algos and Datasets.
StartDates, Remove\_duplicates and Datasets are identical to their
equivalents in the regression section above.

Binary refers to the Y-variable (classification fields) being predicted.
The options for Binary are True and False. If False is selected the
continuous Y-variable is split into 8 categories denoted by the numbers
1-8. These categories represent a number of standard deviations from the
mean of the input Y-variable. See Equation \ref{eq1}. If Binary is set
to True then the Y-variable is split into 2 categories denoted by 1 and
0 which indicate whether the entry is above or below the mean of the
continuous Y variable. See Equation \ref{eq2}.

\begin{align} 
Ycat=   \left\{ 
\begin{array}{ll} 
      1  &\text{where } \mu-2\sigma>Ycont \label{eq1} \\
      2  &\text{where } \mu-\sigma>Ycont>\mu-2\sigma \\
      3  &\text{where } \mu-0.5\sigma>Ycont>\mu-\sigma \\
      4  &\text{where } \mu>Ycont>\mu-0.5\sigma \\
      5  &\text{where } \mu<Ycont<\mu+0.5\sigma \\
      6  &\text{where } \mu+0.5\sigma<Ycont<\mu+\sigma \\
      7  &\text{where } \mu+\sigma<Ycont<\mu+2\sigma \\
      8  &\text{where } \mu+2\sigma<Ycont \\
\end{array} 
\right. 
\end{align} \[
where \ \sigma = standard \ deviation \ of \ Ycont \ and \ \mu = mean \ of \ Ycont
\]

\begin{align}
Ycat=   \left\{ 
\begin{array}{ll} 
      1  &\text{where } Ycont>\mu \label{eq2} \\
      0  &\text{where } Ycont<\mu \\
\end{array} 
\right. 
\end{align} \[
where \ \sigma = standard \ deviation \ of \ Ycont \ and \ \mu = mean \ of \ Ycont
\] Clf\_Types refers to the Time Series cross validation and
Cross-section cross validation methods as described in the regression
section. The options in this category are CS\_Classifier and
TS\_Classifier. Clf\_algos is very similar to the Reg\_algos field
described in the previous section. The algorithms available for
classification are stochastic gradient descent, a 3 hidden layer neural
network, logistic regression and gradient boosting. The hyper-parameters
for each of these fields can be found in Appendix B.2.

\hypertarget{results-and-discussion}{%
\section{Results and discussion}\label{results-and-discussion}}

The results of the models indicated that U.S. Presidential Speeches hold
at least a small amount of predictive power over movements in the S\&P
500 stock index. This conclusion was deduced from the fact that the best
performing datasets in both the date-centred classification analysis and
the date-centred regression analysis included vectorized speech data. It
is also corroborated by the consistent appearance of speech subset
inclusive data in the top ten performing models across both data design
types, and both classification and regression tasks.

The date-centred dataset outperformed the speech-centred dataset by
about 2 percentage points (\textasciitilde0,58 vs \textasciitilde0,6) on
the test set accuracy score for binary classification analysis. Models
using the date-centred dataset also achieved lower MAE's than models
trained and tested on the speech-centred dataset. Interestingly, an
implication of the superior performance of the (smaller) date-centred
dataset is that data with high quality has outperformed data with high
quantity. The section below explains the results in more detail.

\hypertarget{speech-centred-classification-analysis}{%
\subsection{Speech-centred classification
analysis}\label{speech-centred-classification-analysis}}

Analysing the best performing classification models, in terms of test
set accuracy, across all categories of the speech-centred data reveals a
very high likelihood of data contamination in the data containing
duplicate dates. All ten of the top performers are on data containing
duplicates and the accuracies range from 0,72 to 0,80. It is highly
unlikely that these models have managed to achieve 80\% accuracy in
prediction of detrended S\&P 500 on unseen data over a 52 year period.
Thus results of models trained and tested on duplicate date containing
data is discarded henceforth.

Of the duplicate removed results the top ten performers contain seven
models trained and tested on the 2010 dataset -- including the top four.
Further, eight of the top ten models performed binary classification.
Finally seven of the ten were trained on cross-sectional cross
validation. Thus further analysis will take place within the binary,
cross-sectional and 2010 categories. The test set accuracy ranges from
0,54 to 0,58. The mean of the target variable is \textasciitilde{} 0,5
and thus every model in the top ten models outperforms chance (recall
that the target variable is binary). NLP data is included in six of the
ten best models, while auto-regressive data is included in five and meta
data is included in eight of them. Notably an NLP only (gradient
boosting) model ranks fourth with a test set accuracy of 0,56. This
constitutes evidence of the predictive power of US Presidential speeches
over S\&P 500 movements and lends credibility to the hypothesis of this
project.

\hypertarget{date-centred-classification-analysis}{%
\subsection{Date-centred classification
analysis}\label{date-centred-classification-analysis}}

The means of both the full set of 1950-01-01 binary Y's and the test set
of 1950-01-01 binary Y's are maintained at \textasciitilde{} 0,5. This
implies that any accuracy above 0,5 beats chance.

In the date-centred classification results analysis, the top ten models
in terms of test set accuracies contain ten 1950 subsets, ten TS subsets
and eight binary subsets. All of the top ten models utilised
auto-regressive data, five positions included the DBOW vector set and
two included the DM vector set. The range of the top ten models test set
accuracies is 0,57 to 0,59. Thus the date-centred dataset initially
outperforms the (duplicate dates removed) speech-centred dataset by 2
percentage points at the bottom of the range and 1 percentage point at
the top of the range in terms of test set classification accuracy.
Because of this the models trained on the date-centred dataset were
optimised for maximum performance. Log-regression is initially the best
performing classifier in the TS section generally taking the top two and
the seventh positions. However, AutoDBOW outperforms Auto in the Binary
NN and the Binary Gradient Boosting categories. Thus, these three models
were selected for optimisation.

\hypertarget{optimization-of-classification-models-for-ts-classification-of-date-centred-data}{%
\subsubsection{Optimization of classification models for TS
classification of date-centred
data}\label{optimization-of-classification-models-for-ts-classification-of-date-centred-data}}

The training set accuracy on the 1950 binary AutoDBOW for the gradient
boosting classifier was initially 77,5\% indicating that it may have
been slightly overfitting the training data. For the NN the training
accuracy is 59\% which is similar to the test set accuracy and indicates
a good fit for the data. Grid search was manually performed to optimize
NN and Gradient Boosting for the binary 1950 AutoDBOW and Auto datasets.

After closer optimization of hyper-parameters it becomes clear that the
AutoDBOW dataset outperforms the Auto dataset on test score both overall
and across both the Stochastic Gradient Descent and Log Regression
algorithms. The best performing algorithm for the AutoDBOW dataset was
the LogReg\_4 algorithm (please see appendix B for hyperparameters)
which achieved a test set accuracy of 0,601 while the best performing
algorithm for the Auto dataset was the NN\_7 algorithm which achieved a
test set accuracy of 0,599. This difference indicates that there is at
least a slight benefit to including the DBOW dataset in predictive data
and corroborates the evidence presented in section 2.1.

\hypertarget{speech-centred-regression-analysis}{%
\subsection{Speech-centred regression
analysis}\label{speech-centred-regression-analysis}}

The average Mean Absolute Error (MAE) recorded across all 384 regression
models run was 0,87 for the training data, 0,95 for the test data and
0,8 for the validation data. Comparing these with a standard deviation
of 1,26 for the logDif\_date\_resid variable in the 1998 subset shows
that the average absolute error across all the regressions run lies
within one standard deviation of the dependent variable indicating that
the regressions are better predictors of the series than the mean of the
series. This holds across each of the three date subsets (1998, 2000,
and 2010).

The top ten performing models in regression tasks were all trained on
the cross-sectional 2010 dataset and achieved MAE's between 0,68 and
0,7. Interestingly, there was an even split between datasets with
duplicates removed and without. The best performer did not have
duplicates removed whilst the second, third and fourth best performers
did. Analysis of only data with duplicates not removed gave the top four
places to gradient boosting models trained on the AutoMeta, All, Meta
and Auto datasets (in that order). The MAEs for the top four performers
ranged from 0,68 to 0,7. NLP inclusive datasets ranked in five of the
top ten places. Analysis on the duplicates removed top ten performers
shows the NN taking 8 of the top ten places -- including the top seven
spots. NLP inclusive datasets ranked well taking the second, third,
fourth, fifth and seventh spots but not beating the purely
auto-regressive data.

It is difficult to draw a solid conjecture about the predictive power of
the NLP data from these mixed results. However, data contamination in
the duplicate inclusive dataset is likely and the results should
probably be discarded. Given the fair performance of the NLP inclusive
datasets in the duplicates removed dataset, at this point, it remains
likely that the NLP data holds predictive power.

A final analysis of all the regression models trained on only 1998 NLP
data from the speech-centred dataset shows that the top ten performing
models all achieved a MAE below 1,06. This is still below the standard
deviation of 1,26 for the 1998 Y variable (logDif\_date\_resid) and thus
indicates that regression models trained only on the NLP data outperform
the mean of the series as a predictor. This again corroborates evidence
that U.S. Presidential Speeches having predictive power over S\&P 500
movements.

\hypertarget{date-centred-regression-analysis}{%
\subsection{Date-centred regression
analysis}\label{date-centred-regression-analysis}}

The initial ten lowest test set MAE's across all the regression
categories were in the 1950 and cross section categories. The standard
deviation for the 1950 test set is 0.96. All ten of these MAEs clustered
around 0,66 so the regressions are better predictors than the mean.
Again the date-centred dataset has outperformed the speech-centred
dataset. The best score was achieved for the DBOW dataset by the SGD
algorithm. The subsets including NLP data were included in seven of the
top ten performing models and three of them only contained NLP data.
Notably, the AutoDM subset outperformed the Auto subset in the NN model
- undeniably indicating the predictive power of the NLP data.

NN algorithms were responsible for six of the lowest ten MAEs and SGD
models were responsible for a further 3. The DBOW dataset appeared
twice, the Auto dataset 3 times, the DM dataset once, AutoDBOW twice,
AutoDM once and AutoBoth once. In total the Auto dataset appeared in 7
of the top ten performers, the DBOW appeared in 5 and the DM dataset
appeared 3 times. Given this and the predictive ability of the DBOW
shown in the classification section above, DBOW and Auto data sets were
compared across attempted optimizations of the NN and SGD algorithms.
However, no significant improvement could be engineered and almost all
models performed worse than the initial models. Given that the best
performing dataset in terms of test MAE was the DBOW dataset, further
evidence is presented that U.S. presidential Speeches hold predictive
power over the S\&P 500.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\newpage

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-agarwal2020sentiment}{}}%
Agarwal, A. 2020. Sentiment analysis of financial news. In IEEE
\emph{2020 12th international conference on computational intelligence
and communication networks (CICN)}. 312--315.

\leavevmode\vadjust pre{\hypertarget{ref-baker2016measuring}{}}%
Baker, S.R., Bloom, N. \& Davis, S.J. 2016. Measuring economic policy
uncertainty. \emph{The quarterly journal of economics}.
131(4):1593--1636.

\leavevmode\vadjust pre{\hypertarget{ref-biswas2020examining}{}}%
Biswas, S., Sarkar, I., Das, P., Bose, R. \& Roy, S. 2020. Examining the
effects of pandemics on stock market trends through sentiment analysis.
\emph{J Xidian Univ}. 14(6):1163--76.

\leavevmode\vadjust pre{\hypertarget{ref-dilaisentiment}{}}%
Dilai, M., Onukevych, Y. \& Dilai, I. 2021. Sentiment analysis of the US
and ukrainian presidential speeches. \emph{Computational linguistics and
intelligent systems}. 60--70.

\leavevmode\vadjust pre{\hypertarget{ref-elbagir2019twitter}{}}%
Elbagir, S. \& Yang, J. 2019. Twitter sentiment analysis using natural
language toolkit and VADER sentiment. In Vol. 122 \emph{Proceedings of
the international multiconference of engineers and computer scientists}.
16.

\leavevmode\vadjust pre{\hypertarget{ref-geron2019hands}{}}%
Géron, A. 2019. \emph{Hands-on machine learning with scikit-learn,
keras, and TensorFlow: Concepts, tools, and techniques to build
intelligent systems}. " O'Reilly Media, Inc.".

\leavevmode\vadjust pre{\hypertarget{ref-hassani2020selecting}{}}%
Hassani, H. \& Yeganegi, M.R. 2020. Selecting optimal lag order in
ljung--box test. \emph{Physica A: Statistical Mechanics and its
Applications}. 541:123700.

\leavevmode\vadjust pre{\hypertarget{ref-hayo2008communicating}{}}%
Hayo, B., Kutan, A.M. \& Neuenkirch, M. 2008. \emph{Communicating with
many tongues: FOMC speeches and US financial market reaction}. MAGKS
Joint Discussion Paper Series in Economics.

\leavevmode\vadjust pre{\hypertarget{ref-hutto2014vader}{}}%
Hutto, C. \& Gilbert, E. 2014. Vader: A parsimonious rule-based model
for sentiment analysis of social media text. In Vol. 8. (1)
\emph{Proceedings of the international AAAI conference on web and social
media}. 216--225.

\leavevmode\vadjust pre{\hypertarget{ref-hyndman2018forecasting}{}}%
Hyndman, R.J. \& Athanasopoulos, G. 2018. \emph{Forecasting: Principles
and practice}. OTexts.

\leavevmode\vadjust pre{\hypertarget{ref-jiao2017predicting}{}}%
Jiao, Y. \& Jakubowicz, J. 2017. Predicting stock movement direction
with machine learning: An extensive study on s\&p 500 stocks. In IEEE
\emph{2017 IEEE international conference on big data (big data)}.
4705--4713.

\leavevmode\vadjust pre{\hypertarget{ref-katre2019nlp}{}}%
Katre, P.D. 2019. NLP based text analytics and visualization of
political speeches. \emph{International Journal of Recent Technology and
Engineering}. 8(3):8574--8579.

\leavevmode\vadjust pre{\hypertarget{ref-khedr2017predicting}{}}%
Khedr, A.E., Yaseen, N., et al. 2017. Predicting stock market behavior
using data mining technique and news sentiment analysis.
\emph{International Journal of Intelligent Systems and Applications}.
9(7):22.

\leavevmode\vadjust pre{\hypertarget{ref-kinyua2021analysis}{}}%
Kinyua, J.D., Mutigwe, C., Cushing, D.J. \& Poggi, M. 2021. An analysis
of the impact of president trump's tweets on the DJIA and s\&p 500 using
machine learning and sentiment analysis. \emph{Journal of behavioral and
experimental finance}. 29:100447.

\leavevmode\vadjust pre{\hypertarget{ref-kotze2020univariate}{}}%
Kotzé, K. 2020. {[}Online{]}, Available:
\url{https://kevin-kotze.gitlab.io/tsm/ts-8-note/}.

\leavevmode\vadjust pre{\hypertarget{ref-kotze2021volatility}{}}%
Kotzé, K. 2021. {[}Online{]}, Available:
\url{https://kevinkotze.github.io/ts-12-tut/}.

\leavevmode\vadjust pre{\hypertarget{ref-le2014distributed}{}}%
Le, Q. \& Mikolov, T. 2014. Distributed representations of sentences and
documents. In PMLR \emph{International conference on machine learning}.
1188--1196.

\leavevmode\vadjust pre{\hypertarget{ref-liu2016forecasting}{}}%
Liu, C., Wang, J., Xiao, D. \& Liang, Q. 2016. Forecasting s\&p 500
stock index using statistical learning models. \emph{Open journal of
statistics}. 6(6):1067--1075.

\leavevmode\vadjust pre{\hypertarget{ref-loria2018textblob}{}}%
Loria, S. et al. 2018. Textblob documentation. \emph{Release 0.15}.
2(8).

\leavevmode\vadjust pre{\hypertarget{ref-maligkris2017political}{}}%
Maligkris, A. 2017. Political speeches and stock market outcomes. In
\emph{30th australasian finance and banking conference}.

\leavevmode\vadjust pre{\hypertarget{ref-mikolov2013efficient}{}}%
Mikolov, T., Chen, K., Corrado, G. \& Dean, J. 2013. Efficient
estimation of word representations in vector space. \emph{arXiv preprint
arXiv:1301.3781}.

\leavevmode\vadjust pre{\hypertarget{ref-mikolov2013distributed}{}}%
Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S. \& Dean, J. 2013.
Distributed representations of words and phrases and their
compositionality. \emph{Advances in neural information processing
systems}. 26.

\leavevmode\vadjust pre{\hypertarget{ref-pano2020complete}{}}%
Pano, T. \& Kashef, R. 2020. A complete VADER-based sentiment analysis
of bitcoin (BTC) tweets during the era of COVID-19. \emph{Big Data and
Cognitive Computing}. 4(4):33.

\leavevmode\vadjust pre{\hypertarget{ref-purevdagva2020machine}{}}%
Purevdagva, C., Zhao, R., Huang, P.-C. \& Mahoney, W. 2020. A
machine-learning based framework for detection of fake political speech.
In IEEE \emph{2020 IEEE 14th international conference on big data
science and engineering (BigDataSE)}. 80--87.

\leavevmode\vadjust pre{\hypertarget{ref-qin230natural}{}}%
Qin, C.J.J. 2018. Natural language processing and event-driven stock
prediction. {[}Online{]}, Available:
\url{http://cs230.stanford.edu/projects_spring_2018/reports/8290001.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-rees2022mastersgithub}{}}%
Rees, P. 2022. {[}Online{]}, Available:
\url{https://github.com/PabloRees/Masters}.

\leavevmode\vadjust pre{\hypertarget{ref-rehurek2011gensim}{}}%
Rehurek, R. \& Sojka, P. 2011. Gensim--python framework for vector space
modelling. \emph{NLP Centre, Faculty of Informatics, Masaryk University,
Brno, Czech Republic}. 3(2).

\leavevmode\vadjust pre{\hypertarget{ref-ren2018forecasting}{}}%
Ren, R., Wu, D.D. \& Liu, T. 2018. Forecasting stock market movement
direction using sentiment analysis and support vector machine.
\emph{IEEE Systems Journal}. 13(1):760--770.

\leavevmode\vadjust pre{\hypertarget{ref-sazedj2011hope}{}}%
Sazedj, S. \& Tavares, J. 2011. Hope, change, and financial markets: Can
obama's words drive the market? \emph{Available at SSRN 1976054}.

\leavevmode\vadjust pre{\hypertarget{ref-shi2019word2vec}{}}%
Shi, B., Zhao, J. \& Xu, K. 2019. A Word2vec model for sentiment
analysis of weibo. In IEEE \emph{2019 16th international conference on
service systems and service management (ICSSSM)}. 1--6.

\leavevmode\vadjust pre{\hypertarget{ref-sohangir2018financial}{}}%
Sohangir, S., Petty, N. \& Wang, D. 2018. Financial sentiment lexicon
analysis. In IEEE \emph{2018 IEEE 12th international conference on
semantic computing (ICSC)}. 286--289.

\leavevmode\vadjust pre{\hypertarget{ref-vargas2017deep}{}}%
Vargas, M.R., De Lima, B.S. \& Evsukoff, A.G. 2017. Deep learning for
stock market prediction from financial news articles. In IEEE \emph{2017
IEEE international conference on computational intelligence and virtual
environments for measurement systems and applications (CIVEMSA)}.
60--65.

\leavevmode\vadjust pre{\hypertarget{ref-americanPresProj1}{}}%
Woolley, J. \& Peters, G. n.d. {[}Online{]}, Available:
\url{https://www.presidency.ucsb.edu/}.

\leavevmode\vadjust pre{\hypertarget{ref-yahooSP500}{}}%
Yahoo. n.d. {[}Online{]}, Available:
\url{https://finance.yahoo.com/quote/\%5EGSPC/history?period1=-\%201325635200\&period2=1641340800\&interval=1d\&filter=history\&frequency=1d\&includeAdj\%20ustedClose=true}.

\leavevmode\vadjust pre{\hypertarget{ref-zubair2015extracting}{}}%
Zubair, S. \& Cios, K.J. 2015. Extracting news sentiment and
establishing its relationship with the s\&p 500 index. In IEEE
\emph{2015 48th hawaii international conference on system sciences}.
969--975.

\end{CSLReferences}

\hypertarget{appendix}{%
\section*{Appendix}\label{appendix}}
\addcontentsline{toc}{section}{Appendix}

\hypertarget{appendix-a}{%
\subsection*{Appendix A}\label{appendix-a}}
\addcontentsline{toc}{subsection}{Appendix A}

Some appendix information here

\hypertarget{appendix-b}{%
\subsection*{Appendix B}\label{appendix-b}}
\addcontentsline{toc}{subsection}{Appendix B}

\bibliography{Tex/ref}





\end{document}
